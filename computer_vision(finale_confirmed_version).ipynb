{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Computer Vision – Homework\n",
        "## Fait par : Hatim SALMI , Taha ELMARZOUKI "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " To accomplish this task, we will need to follow these steps:\n",
        "\n",
        "1.Data Preparation: We load the CIFAR-10 dataset and label the classes appropriately.\n",
        "\n",
        "2.Model Architecture: We design a Convolutional Neural Network (CNN) suitable for binary classification.\n",
        "\n",
        "3.Loss Function: We use an appropriate loss function for binary classification.\n",
        "\n",
        "4.Model Training: We train the model on the training set.\n",
        "\n",
        "5.Evaluation: We evaluate the model using precision, recall, F1 score, accuracy, and a confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Data Preparation\n",
        "## We load the CIFAR-10 dataset and create labels for the two categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we transform the multiclass labels from the CIFAR-10 dataset into binary labels. Our goal is to group the CIFAR-10 classes into two categories: \"can fly\" and \"cannot fly\". Here’s a detailed explanation of the process, including the code.\n",
        "\n",
        "\n",
        "-Before running any TensorFlow code, we ensure that TensorFlow is installed.\n",
        "\n",
        "-We import all the necessary libraries, including TensorFlow and other utilities for handling data, building the model, and evaluating performance.\n",
        "\n",
        "-We load the CIFAR-10 dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class.\n",
        "\n",
        "-We define the classes that can fly (airplane and bird) and those that cannot fly (the rest):\n",
        "    The CIFAR-10 dataset consists of 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n",
        "    We need to classify these into two groups:\n",
        "        Can Fly: Includes 'airplane' and 'bird' (class indices 0 and 2 in CIFAR-10).\n",
        "        Cannot Fly: Includes all other classes (indices 1, 3, 4, 5, 6, 7, 8, 9 in CIFAR-10).\n",
        "\n",
        "-We then create binary labels for these classes:\n",
        "    The original labels (y_train and y_test) are integers from 0 to 9, representing the 10 classes.\n",
        "    We convert these into binary labels where:\n",
        "        1 indicates the class belongs to the \"can fly\" group.\n",
        "        0 indicates the class belongs to the \"cannot fly\" group.\n",
        "    This is done by checking if each label is in the list of classes that can fly (fly_classes).\n",
        "\n",
        "-We normalize the image data to the range [0, 1] for better training performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd1r6Mq5sFiv",
        "outputId": "ffa287f7-a33e-4685-8874-6946eab6a498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Install TensorFlow\n",
        "!pip install tensorflow\n",
        "\n",
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Define the classes that can and cannot fly\n",
        "fly_classes = [0, 2]  # airplane, bird\n",
        "not_fly_classes = [1, 3, 4, 5, 6, 7, 8, 9]  # the rest\n",
        "\n",
        "# Create binary labels\n",
        "def create_binary_labels(y, fly_classes):\n",
        "    y_binary = np.isin(y, fly_classes).astype(int)\n",
        "    return y_binary\n",
        "\n",
        "y_train_binary = create_binary_labels(y_train, fly_classes)\n",
        "y_test_binary = create_binary_labels(y_test, fly_classes)\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Model Architecture\n",
        "## We design a simple CNN for binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.We build a Convolutional Neural Network (CNN) with several layers, including convolutional, max-pooling, dense, and dropout layers. The output layer uses a sigmoid activation function for binary classification.\n",
        "\n",
        "2.We compile the model using the Adam optimizer, binary cross-entropy loss function (suitable for binary classification), and accuracy as the evaluation metric.\n",
        "\n",
        "Explanation: \n",
        "\n",
        "CNN Layers:\n",
        "\n",
        "    Convolutional Layers: These layers apply a set of filters to the input image, capturing various features such as edges, textures, and patterns.\n",
        "    Max Pooling Layers: These layers reduce the spatial dimensions of the feature maps, making the model computationally efficient and reducing overfitting.\n",
        "    Flatten Layer: This layer converts the 2D feature maps into a 1D vector, which can be fed into fully connected layers.\n",
        "    Dense Layers: These layers perform the final classification based on the features extracted by the convolutional layers.\n",
        "    Dropout Layer: This layer randomly sets a fraction of input units to 0 during training to prevent overfitting.\n",
        "    Output Layer: For binary classification, this layer uses a sigmoid activation function to produce a probability score between 0 and 1.\n",
        "\n",
        "Activation Functions:\n",
        "\n",
        "    ReLU (Rectified Linear Unit): This activation function introduces non-linearity to the model, allowing it to learn complex patterns.\n",
        "\n",
        "Optimizer and Loss Function:\n",
        "\n",
        "    The use of binary cross-entropy loss in this CNN training process ensures that the model learns to distinguish between the two classes based on the input images. This loss function, combined with the Adam optimizer and accuracy metric, provides a robust framework for training and evaluating the model on the binary classification task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXnji8dQtstp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    # First Convolutional Layer\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)), # 32 filters, each with a size of 3x3 and we apply the ReLU activation function.\n",
        "    # First Max Pooling Layer\n",
        "    MaxPooling2D((2, 2)), # Reduces the spatial dimensions by a factor of 2\n",
        "    # Second Convolutional Layer\n",
        "    Conv2D(64, (3, 3), activation='relu'), # 64 filters, each with a size of 3x3 and we apply the ReLU activation function.\n",
        "    # Second Max Pooling Layer\n",
        "    MaxPooling2D((2, 2)), # Reduces the spatial dimensions by a factor of 2\n",
        "    # Third Convolutional Layer\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    # Third Max Pooling Layer\n",
        "    MaxPooling2D((2, 2)),\n",
        "    # Flatten Layer\n",
        "    Flatten(), # Converts the 3D feature maps into 1D feature vectors.\n",
        "    # First Dense Layer\n",
        "    Dense(128, activation='relu'),\n",
        "    # Dropout Layer\n",
        "    Dropout(0.5), # Randomly sets 50% of the input units to 0 during each update to prevent overfitting.\n",
        "    # Output Layer\n",
        "    Dense(1, activation='sigmoid') # Applies the sigmoid activation function to produce a probability score for binary classification.\n",
        "])\n",
        "#  Uses binary cross-entropy as the loss function which is suitable for binary classification tasks.\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Model Training\n",
        "## We train the model on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We train the model on the training data, using 20% of it for validation. The model is trained for 20 epochs with a batch size of 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVI8E2aotx9-",
        "outputId": "cc0e8ef0-d3f1-4e78-dd20-f5815b11eeb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 58s 90ms/step - loss: 0.4315 - accuracy: 0.8192 - val_loss: 0.3731 - val_accuracy: 0.8452\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 55s 89ms/step - loss: 0.3604 - accuracy: 0.8524 - val_loss: 0.3279 - val_accuracy: 0.8658\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 59s 94ms/step - loss: 0.3238 - accuracy: 0.8705 - val_loss: 0.3167 - val_accuracy: 0.8718\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 56s 89ms/step - loss: 0.2951 - accuracy: 0.8820 - val_loss: 0.3017 - val_accuracy: 0.8800\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 55s 88ms/step - loss: 0.2729 - accuracy: 0.8916 - val_loss: 0.2894 - val_accuracy: 0.8841\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 54s 86ms/step - loss: 0.2517 - accuracy: 0.9001 - val_loss: 0.2808 - val_accuracy: 0.8866\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 55s 88ms/step - loss: 0.2259 - accuracy: 0.9119 - val_loss: 0.2883 - val_accuracy: 0.8934\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 55s 88ms/step - loss: 0.2037 - accuracy: 0.9200 - val_loss: 0.2942 - val_accuracy: 0.8855\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 58s 92ms/step - loss: 0.1860 - accuracy: 0.9275 - val_loss: 0.2956 - val_accuracy: 0.8911\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 57s 91ms/step - loss: 0.1637 - accuracy: 0.9354 - val_loss: 0.3111 - val_accuracy: 0.8962\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 58s 92ms/step - loss: 0.1408 - accuracy: 0.9458 - val_loss: 0.3474 - val_accuracy: 0.8910\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 55s 88ms/step - loss: 0.1256 - accuracy: 0.9520 - val_loss: 0.3568 - val_accuracy: 0.8865\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 57s 92ms/step - loss: 0.1137 - accuracy: 0.9556 - val_loss: 0.3771 - val_accuracy: 0.8854\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 60s 95ms/step - loss: 0.0967 - accuracy: 0.9618 - val_loss: 0.3924 - val_accuracy: 0.8862\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 55s 89ms/step - loss: 0.0828 - accuracy: 0.9686 - val_loss: 0.4045 - val_accuracy: 0.8854\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 56s 89ms/step - loss: 0.0743 - accuracy: 0.9716 - val_loss: 0.4592 - val_accuracy: 0.8757\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 55s 88ms/step - loss: 0.0679 - accuracy: 0.9739 - val_loss: 0.5247 - val_accuracy: 0.8887\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 56s 90ms/step - loss: 0.0610 - accuracy: 0.9765 - val_loss: 0.5272 - val_accuracy: 0.8710\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 56s 90ms/step - loss: 0.0566 - accuracy: 0.9786 - val_loss: 0.5352 - val_accuracy: 0.8783\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 55s 88ms/step - loss: 0.0499 - accuracy: 0.9809 - val_loss: 0.5711 - val_accuracy: 0.8830\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train_binary, epochs=20, batch_size=64, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis & Interpretation:\n",
        "\n",
        "    1.Epochs 1-6:\n",
        "\n",
        "        The model shows a steady decrease in training loss and validation loss, with both metrics improving significantly.\n",
        "        Training accuracy and validation accuracy both increase, indicating that the model is learning effectively.\n",
        "        By Epoch 6, the training loss is 0.2517, and the validation loss is 0.2808, with training accuracy at 90.01% and validation accuracy at 88.66%.\n",
        "\n",
        "    2.Epochs 7-10:\n",
        "\n",
        "        The training loss continues to decrease, and training accuracy continues to increase, reaching 93.54% by Epoch 10.\n",
        "        However, the validation loss begins to fluctuate and increase slightly, while validation accuracy shows minor improvements but then starts to plateau.\n",
        "        By Epoch 10, the training loss is 0.1637, and the validation loss is 0.3111, with training accuracy at 93.54% and validation accuracy at 89.62%.\n",
        "\n",
        "    3.Epochs 11-20:\n",
        "\n",
        "        The model continues to improve its performance on the training set, with training loss dropping to 0.0499 and training accuracy reaching 98.09% by Epoch 20.\n",
        "        The validation loss, however, starts to increase more noticeably, and validation accuracy shows signs of slight decline or plateauing.\n",
        "        By Epoch 20, the training loss is 0.0499, and the validation loss is 0.5711, with training accuracy at 98.09% and validation accuracy at 88.30%.\n",
        "\n",
        "    \n",
        "     Interpretation of Results\n",
        "\n",
        "    Initial Learning (Epochs 1-6):\n",
        "\n",
        "        The model shows effective learning, with both training and validation losses decreasing and accuracies increasing. This indicates that the model is able to generalize well to the validation set initially.\n",
        "\n",
        "    Overfitting (Epochs 7-20):\n",
        "\n",
        "        Starting around Epoch 7, the validation loss begins to increase while the training loss continues to decrease. This suggests that the model is starting to overfit the training data.\n",
        "        Overfitting occurs when the model learns to perform very well on the training data but fails to generalize to new, unseen data. This is evident as the validation loss increases despite the high training accuracy.\n",
        "\n",
        "    Validation Accuracy Plateau:\n",
        "\n",
        "        The validation accuracy plateaus around 88-89%, indicating that the model has reached its generalization capacity given the current architecture and training setup.\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4: Evaluation\n",
        "## We evaluate the model using various metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model is compiled with the binary cross-entropy loss function and trained on the training set.\n",
        "\n",
        "We evaluate the trained model on the test data, calculating predictions and then computing evaluation metrics such as precision, recall, F1 score, and accuracy. We also generate a confusion matrix.\n",
        "\n",
        "In the code below , for compiling with the appropriate loss function and optimizer we use model.compile() , for evaluating the model , we use model.evaluate() to calculate the test loss and test accuracy and then we print the test loss and accuracy.Also for making predictions , we use model.predict() to get the predicted probabilities for the test set.\n",
        "\n",
        "And finally we calculate the metrics .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d7wokNh78jj",
        "outputId": "3f901073-b9e3-43ce-91e6-e384057d200a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 55s 86ms/step - loss: 0.4323 - accuracy: 0.8168 - val_loss: 0.3926 - val_accuracy: 0.8412\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 53s 86ms/step - loss: 0.3668 - accuracy: 0.8493 - val_loss: 0.3572 - val_accuracy: 0.8462\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 51s 82ms/step - loss: 0.3261 - accuracy: 0.8684 - val_loss: 0.3288 - val_accuracy: 0.8680\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 53s 85ms/step - loss: 0.3042 - accuracy: 0.8771 - val_loss: 0.3075 - val_accuracy: 0.8752\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 53s 85ms/step - loss: 0.2834 - accuracy: 0.8858 - val_loss: 0.2940 - val_accuracy: 0.8803\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 53s 85ms/step - loss: 0.2671 - accuracy: 0.8916 - val_loss: 0.2922 - val_accuracy: 0.8819\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 53s 85ms/step - loss: 0.2470 - accuracy: 0.9023 - val_loss: 0.2929 - val_accuracy: 0.8797\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 54s 87ms/step - loss: 0.2305 - accuracy: 0.9095 - val_loss: 0.3152 - val_accuracy: 0.8817\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 53s 84ms/step - loss: 0.2202 - accuracy: 0.9124 - val_loss: 0.2857 - val_accuracy: 0.8859\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 53s 84ms/step - loss: 0.1997 - accuracy: 0.9222 - val_loss: 0.2870 - val_accuracy: 0.8863\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 52s 83ms/step - loss: 0.1854 - accuracy: 0.9266 - val_loss: 0.3027 - val_accuracy: 0.8879\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 53s 85ms/step - loss: 0.1735 - accuracy: 0.9305 - val_loss: 0.3279 - val_accuracy: 0.8752\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 53s 85ms/step - loss: 0.1539 - accuracy: 0.9397 - val_loss: 0.3142 - val_accuracy: 0.8851\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 53s 85ms/step - loss: 0.1399 - accuracy: 0.9455 - val_loss: 0.3236 - val_accuracy: 0.8793\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 54s 87ms/step - loss: 0.1263 - accuracy: 0.9515 - val_loss: 0.3504 - val_accuracy: 0.8876\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 55s 87ms/step - loss: 0.1141 - accuracy: 0.9559 - val_loss: 0.3567 - val_accuracy: 0.8841\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 53s 85ms/step - loss: 0.0974 - accuracy: 0.9620 - val_loss: 0.3983 - val_accuracy: 0.8841\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 53s 84ms/step - loss: 0.0884 - accuracy: 0.9667 - val_loss: 0.4210 - val_accuracy: 0.8878\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 53s 85ms/step - loss: 0.0748 - accuracy: 0.9709 - val_loss: 0.4418 - val_accuracy: 0.8837\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 54s 87ms/step - loss: 0.0688 - accuracy: 0.9735 - val_loss: 0.4672 - val_accuracy: 0.8789\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.4624 - accuracy: 0.8784\n",
            "Test loss: 0.4624193012714386\n",
            "Test accuracy: 0.8784000277519226\n",
            "313/313 [==============================] - 4s 12ms/step\n",
            "Precision: 0.697979797979798\n",
            "Recall: 0.691\n",
            "F1 Score: 0.6944723618090453\n",
            "Confusion Matrix:\n",
            "[[7402  598]\n",
            " [ 618 1382]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train_full, y_train_full), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to range [0, 1]\n",
        "x_train_full = x_train_full.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Create binary labels for full training set\n",
        "y_train_full_binary = np.where((y_train_full == 0) | (y_train_full == 2), 1, 0)\n",
        "\n",
        "# Split full training set into training and validation sets (80-20 split for training and testing)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define CNN architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model with appropriate loss function and optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test_binary)\n",
        "print(f'Test loss: {test_loss}')\n",
        "print(f'Test accuracy: {test_accuracy}')\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_test_pred = model.predict(x_test)\n",
        "y_test_pred_binary = (y_test_pred > 0.5).astype(int)  #if the predicted probability is greater than 0.5, it will evaluate to True; otherwise,false.\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_test_binary, y_test_pred_binary)\n",
        "recall = recall_score(y_test_binary, y_test_pred_binary)\n",
        "f1 = f1_score(y_test_binary, y_test_pred_binary)\n",
        "conf_matrix = confusion_matrix(y_test_binary, y_test_pred_binary)\n",
        "\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation\n",
        "Training Performance:\n",
        "    Loss and Accuracy:\n",
        "        The training loss gradually decreases from approximately 0.43 to 0.07 over the 20 epochs, indicating that the model's predictions are improving.\n",
        "        The training accuracy increases from around 0.82 to 0.97, showing that the model is learning to classify the training data more accurately with each epoch.\n",
        "\n",
        "Validation Performance:\n",
        "    Loss and Accuracy:\n",
        "        The validation loss decreases from about 0.39 to 0.47, but it slightly increases after around the 10th epoch. This might indicate overfitting, where the model starts to memorize the training data instead of generalizing well to unseen data.\n",
        "        The validation accuracy reaches a peak of around 0.89 after approximately the 9th epoch but then starts to decrease slightly, similar to the validation loss trend.\n",
        "\n",
        "Testing Performance:\n",
        "    Loss and Accuracy:\n",
        "        The test loss is approximately 0.46, and the test accuracy is about 0.88. This indicates that the model performs similarly well on unseen test data as it did on the validation data.\n",
        "\n",
        "Precision, Recall, and F1 Score:\n",
        "    Precision: Precision is around 0.70, indicating that when the model predicts a sample to belong to the \"can fly\" category, it is correct about 70% of the time.\n",
        "    Recall: Recall is approximately 0.69, indicating that the model correctly identifies about 69% of the samples belonging to the \"can fly\" category.\n",
        "    F1 Score: The F1 score is about 0.69, which is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
        "\n",
        "Confusion Matrix:\n",
        "    The confusion matrix shows the following:\n",
        "        True Negative (TN): 7402 samples were correctly classified as \"cannot fly\".\n",
        "        False Positive (FP): 598 samples were incorrectly classified as \"can fly\" when they actually cannot.\n",
        "        False Negative (FN): 618 samples were incorrectly classified as \"cannot fly\" when they actually can.\n",
        "        True Positive (TP): 1382 samples were correctly classified as \"can fly\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Suggestions to possibly improve performance\n",
        "\n",
        "Model Architecture:\n",
        "\n",
        "    We may need to experiment with different CNN architectures, including variations in the number of layers, kernel sizes, and filter depths.\n",
        "\n",
        "Data Augmentation:\n",
        "\n",
        "    We may need to apply data augmentation techniques such as rotation, flipping, scaling, and shifting to artificially increase the size of the training dataset and improve model generalization.\n",
        "\n",
        "Hyperparameter Tuning:\n",
        "\n",
        "We may need to search for optimal hyperparameters such as learning rate and batch size.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
